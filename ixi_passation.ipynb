{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1fc5aa9-e982-4fc7-911d-ab05dc6be668",
   "metadata": {},
   "source": [
    "Objet : EGMS — extraction zone + décomposition ASC/DESC + cumuls + extrêmes + export TS\n",
    "\n",
    "Ce notebook produit, pour une zone d’intérêt (AOI) définie par un polygone KML + l’emprise du bâtiment, une extraction EGMS L2a et une décomposition ASC/DESC permettant d’estimer deux composantes de vitesse :\n",
    "\n",
    "Vv : verticale (mm/an)\n",
    "\n",
    "Ve : Est–Ouest (mm/an)\n",
    "\n",
    "Entrées\n",
    "\n",
    "ixi_rennes.kml : polygone AOI (WGS84) utilisé pour contraindre le bord Sud.\n",
    "\n",
    "building_roi.gpkg : empreinte bâtiment (EPSG:2154 attendu) utilisée pour N/E/W.\n",
    "\n",
    "geographic_extents_nicol.gpkg : index des tuiles EGMS (EPSG:3035).\n",
    "\n",
    "Répertoire EGMS local (base_dir) contenant les CSV tuiles L2a.\n",
    "\n",
    "Sorties (dossier egms_extract_zone/)\n",
    "\n",
    "pids/ : PIDs extraits par tuile dans l’AOI (chunked).\n",
    "\n",
    "attr/ : attributs EGMS filtrés par PID (chunked).\n",
    "\n",
    "figs/ : cartes (Vv, Ve, cumuls ΔV, ΔE) + carte+TS des extrêmes.\n",
    "\n",
    "outputs.gpkg :\n",
    "\n",
    "vel_mm_per_year : points “paires” ASC/DESC avec Vv, Ve, cond, dist_m\n",
    "\n",
    "cum_mm : cumuls ΔV, ΔE (approx = V × durée)\n",
    "\n",
    "building_roi : empreinte bâtiment\n",
    "\n",
    "Méthode (résumé)\n",
    "\n",
    "AOI hybride : bbox avec Sud issu du KML, et N/E/W issus du bâtiment reprojeté en EPSG:3035.\n",
    "\n",
    "Sélection des tuiles : intersection bbox AOI vs extents EGMS.\n",
    "\n",
    "Extraction PIDs : lecture CSV par chunks, filtre bbox puis filtre polygonal.\n",
    "\n",
    "Extraction attributs : pour chaque tuile, lecture par chunks + filtre sur set de PIDs.\n",
    "\n",
    "Pairing ASC/DESC : jointure nearest (max_distance=5 m) puis réduction 1–1 par distance minimale.\n",
    "\n",
    "Décomposition : résolution d’un système 2x2 (LOS ASC/DESC → Ve, Vv) avec filtre conditionnement cond <= 50.\n",
    "\n",
    "Cumuls : ΔV = Vv × durée, ΔE = Ve × durée (durées paramétrables).\n",
    "\n",
    "Analyse extrêmes : repérage du point de cumul vertical max(|ΔV|) par période + export des TS LOS (ASC/DESC) associées, avec superposition chaînée par offset médian sur recouvrements.\n",
    "\n",
    "Paramètres clés à vérifier selon site\n",
    "\n",
    "MAX_DIST_M (par défaut 5 m), COND_MAX (50)\n",
    "\n",
    "DURATION_Y (7 ans / 5 ans ici) : à ajuster si besoin selon bornes exactes\n",
    "\n",
    "orbit_from_tile_safe() : mapping tuile → ASC/DESC (spécifique au cas)\n",
    "\n",
    "Unités des séries temporelles exportées : ici on exporte les colonnes dates du CSV EGMS (LOS), à documenter selon produit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25282007-0e8b-4171-aff6-59b328b10fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import contextily as ctx\n",
    "\n",
    "from shapely.geometry import Polygon, Point, box\n",
    "from shapely.prepared import prep\n",
    "from pyproj import Transformer\n",
    "from geopandas import sjoin_nearest\n",
    "\n",
    "# ============================================================\n",
    "# 0) PARAMÈTRES\n",
    "# ============================================================\n",
    "VERBOSE = True\n",
    "\n",
    "# --- Inputs ---\n",
    "KML_PATH = \"ixi_rennes.kml\"\n",
    "BUILDING_GPKG = \"building_roi.gpkg\"  # CRS 2154 attendu\n",
    "EXTENTS_GPKG_NAME = \"geographic_extents_nicol.gpkg\"\n",
    "\n",
    "BASE_DIR_CANDIDATES = [\n",
    "    \"/mnt/c/Users/DataScience/OneDrive - jll.spear/Documents/EGMS/\",\n",
    "    \"/mnt/c/Users/nicol/OneDrive - jll.spear/Documents/EGMS/\",\n",
    "]\n",
    "\n",
    "# --- Outputs ---\n",
    "OUT_DIR = \"egms_extract_zone\"\n",
    "OUT_PIDS = os.path.join(OUT_DIR, \"pids\")\n",
    "OUT_ATTR = os.path.join(OUT_DIR, \"attr\")\n",
    "OUT_FIGS = os.path.join(OUT_DIR, \"figs\")\n",
    "OUT_GPKG = os.path.join(OUT_DIR, \"outputs.gpkg\")\n",
    "\n",
    "# --- Processing ---\n",
    "CHUNKSIZE = 200_000\n",
    "PERIODS = [\"2015_2021\", \"2018_2022\", \"2019_2023\"]\n",
    "\n",
    "# pairing ASC/DESC\n",
    "MAX_DIST_M = 5.0\n",
    "COND_MAX = 50.0\n",
    "\n",
    "# cumuls\n",
    "DURATION_Y = {\n",
    "    \"2015_2021\": 7.0,  # ajuste si besoin (années pleines vs bornes exactes)\n",
    "    \"2018_2022\": 5.0,\n",
    "    \"2019_2023\": 5.0,\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Mapping \"tile_safe -> orbit/period\" (à adapter si besoin)\n",
    "# ------------------------------------------------------------\n",
    "def orbit_from_tile_safe(safe: str) -> str:\n",
    "    if \"030_0283\" in safe or \"030_0284\" in safe:\n",
    "        return \"ASC\"\n",
    "    if \"081_0789\" in safe or \"154_0787\" in safe:\n",
    "        return \"DESC\"\n",
    "    return \"UNK\"\n",
    "\n",
    "def period_from_tile_safe(safe: str) -> str:\n",
    "    if \"2018_2022\" in safe:\n",
    "        return \"2018_2022\"\n",
    "    if \"2019_2023\" in safe:\n",
    "        return \"2019_2023\"\n",
    "    return \"2015_2021\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) UTILITAIRES (prints + checks)\n",
    "# ============================================================\n",
    "def vprint(*args):\n",
    "    if VERBOSE:\n",
    "        print(*args)\n",
    "\n",
    "def ensure_dir(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    vprint(\"[DIR]\", path)\n",
    "\n",
    "def assert_exists(path, label=\"file\"):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Missing {label}: {path}\")\n",
    "    vprint(\"[OK]\", label, \"exists:\", path)\n",
    "\n",
    "def summarize_gdf(gdf, name=\"gdf\"):\n",
    "    vprint(f\"\\n[{name}] n={len(gdf)} cols={list(gdf.columns)}\")\n",
    "    vprint(f\"[{name}] crs={gdf.crs}\")\n",
    "    try:\n",
    "        vprint(f\"[{name}] bounds={gdf.total_bounds}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def summarize_df(df, name=\"df\", head=3):\n",
    "    vprint(f\"\\n[{name}] shape={df.shape}\")\n",
    "    vprint(f\"[{name}] columns={list(df.columns)}\")\n",
    "    vprint(df.head(head))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) AOI : KML (sud) + bâtiment (N/E/W)\n",
    "# ============================================================\n",
    "def find_base_dir(candidates):\n",
    "    for p in candidates:\n",
    "        if os.path.exists(p):\n",
    "            vprint(\"[BASE_DIR]\", p)\n",
    "            return p\n",
    "    raise FileNotFoundError(\"No EGMS base_dir found in BASE_DIR_CANDIDATES.\")\n",
    "\n",
    "def parse_kml_polygon_wgs84(kml_path) -> Polygon:\n",
    "    ns = {\"kml\": \"http://www.opengis.net/kml/2.2\"}\n",
    "    root = ET.parse(kml_path).getroot()\n",
    "    node = root.find(\".//kml:Polygon/kml:outerBoundaryIs/kml:LinearRing/kml:coordinates\", ns)\n",
    "    if node is None or node.text is None:\n",
    "        raise ValueError(\"No Polygon coordinates found in KML.\")\n",
    "    coords = []\n",
    "    for token in node.text.strip().split():\n",
    "        lon, lat = token.split(\",\")[:2]\n",
    "        coords.append((float(lon), float(lat)))\n",
    "    poly = Polygon(coords)\n",
    "    if not poly.is_valid:\n",
    "        vprint(\"[WARN] KML polygon invalid -> buffer(0) fix attempt\")\n",
    "        poly = poly.buffer(0)\n",
    "    return poly\n",
    "\n",
    "def project_polygon(poly4326: Polygon, dst_epsg=3035) -> Polygon:\n",
    "    tr = Transformer.from_crs(\"EPSG:4326\", f\"EPSG:{dst_epsg}\", always_xy=True)\n",
    "    xs, ys = poly4326.exterior.coords.xy\n",
    "    X, Y = tr.transform(xs, ys)\n",
    "    poly = Polygon(zip(X, Y))\n",
    "    if not poly.is_valid:\n",
    "        vprint(\"[WARN] projected polygon invalid -> buffer(0) fix attempt\")\n",
    "        poly = poly.buffer(0)\n",
    "    return poly\n",
    "\n",
    "def build_hybrid_aoi_bbox_3035(building_gpkg, kml_path):\n",
    "    assert_exists(building_gpkg, \"BUILDING_GPKG\")\n",
    "    assert_exists(kml_path, \"KML_PATH\")\n",
    "\n",
    "    # KML -> 3035\n",
    "    poly_kml_3035 = project_polygon(parse_kml_polygon_wgs84(kml_path), 3035)\n",
    "    xmin_k, ymin_k, xmax_k, ymax_k = poly_kml_3035.bounds\n",
    "    vprint(\"[AOI] KML_3035 bounds:\", (xmin_k, ymin_k, xmax_k, ymax_k))\n",
    "\n",
    "    # building -> 3035\n",
    "    b = gpd.read_file(building_gpkg)\n",
    "    summarize_gdf(b, \"building_raw\")\n",
    "    b3035 = b.to_crs(3035)\n",
    "    summarize_gdf(b3035, \"building_3035\")\n",
    "    xmin_b, ymin_b, xmax_b, ymax_b = b3035.total_bounds\n",
    "    vprint(\"[AOI] BUILDING_3035 bounds:\", (xmin_b, ymin_b, xmax_b, ymax_b))\n",
    "\n",
    "    # AOI bbox : S = KML ; N/E/W = building\n",
    "    poly3035 = box(xmin_b, ymin_k, xmax_b, ymax_b)\n",
    "\n",
    "    vprint(\"[AOI] hybrid bbox3035:\", poly3035.bounds)\n",
    "    vprint(\"[AOI] hybrid area (m²):\", poly3035.area)\n",
    "\n",
    "    bbox3035 = box(*poly3035.bounds)\n",
    "    return b3035, poly3035, bbox3035, prep(poly3035)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) Sélection des tuiles EGMS qui intersectent bbox AOI\n",
    "# ============================================================\n",
    "def select_tiles(extents_gpkg_path, bbox3035):\n",
    "    assert_exists(extents_gpkg_path, \"EXTENTS_GPKG\")\n",
    "    ext = gpd.read_file(extents_gpkg_path)\n",
    "    summarize_gdf(ext, \"extents_raw\")\n",
    "\n",
    "    # important: extents en général déjà en 3035 (à vérifier)\n",
    "    vprint(\"[EXTENTS] CRS:\", ext.crs)\n",
    "\n",
    "    tile_boxes = gpd.GeoSeries(\n",
    "        [box(r[\"Min Easting\"], r[\"Min Northing\"], r[\"Max Easting\"], r[\"Max Northing\"]) for _, r in ext.iterrows()],\n",
    "        crs=ext.crs\n",
    "    )\n",
    "    ext = ext.assign(_tile_geom=tile_boxes)\n",
    "    sel = ext[ext[\"_tile_geom\"].intersects(bbox3035)].drop(columns=[\"_tile_geom\"]).copy()\n",
    "\n",
    "    vprint(\"[TILES] intersect AOI:\", len(sel), \"/\", len(ext))\n",
    "    return sel\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) Extraction PID dans AOI par tuile (chunk + bbox + polygon)\n",
    "# ============================================================\n",
    "def extract_zone_pids(big_csv_path, out_csv, bbox3035, poly3035_prepared, chunksize=200_000):\n",
    "    usecols = [\"pid\", \"easting\", \"northing\"]\n",
    "    dtypes  = {\"pid\": \"string\", \"easting\": \"float64\", \"northing\": \"float64\"}\n",
    "\n",
    "    if os.path.exists(out_csv):\n",
    "        os.remove(out_csv)\n",
    "\n",
    "    xmin, ymin, xmax, ymax = bbox3035.bounds\n",
    "    first, kept, read_total = True, 0, 0\n",
    "\n",
    "    for chunk in pd.read_csv(big_csv_path, usecols=usecols, dtype=dtypes, chunksize=chunksize):\n",
    "        read_total += len(chunk)\n",
    "\n",
    "        m = (chunk[\"easting\"].between(xmin, xmax) & chunk[\"northing\"].between(ymin, ymax))\n",
    "        sub = chunk.loc[m]\n",
    "        if sub.empty:\n",
    "            continue\n",
    "\n",
    "        keep = [poly3035_prepared.contains(Point(x, y)) for x, y in zip(sub[\"easting\"].to_numpy(), sub[\"northing\"].to_numpy())]\n",
    "        sub = sub.loc[keep]\n",
    "        if sub.empty:\n",
    "            continue\n",
    "\n",
    "        sub.to_csv(out_csv, mode=\"w\" if first else \"a\", index=False, header=first)\n",
    "        first = False\n",
    "        kept += len(sub)\n",
    "\n",
    "    return kept, read_total\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) Extraction attributs pour PID (chunk + filtre set)\n",
    "# ============================================================\n",
    "def extract_attrs_for_pids(big_csv_path, pids_csv_path, out_attr_csv, chunksize=200_000):\n",
    "    pids = set(pd.read_csv(pids_csv_path, usecols=[\"pid\"])[\"pid\"].astype(str))\n",
    "    if not pids:\n",
    "        return 0, 0\n",
    "\n",
    "    usecols = [\"pid\",\"easting\",\"northing\",\"los_east\",\"los_up\",\"mean_velocity\",\"temporal_coherence\"]\n",
    "\n",
    "    if os.path.exists(out_attr_csv):\n",
    "        os.remove(out_attr_csv)\n",
    "\n",
    "    first, kept, read_total = True, 0, 0\n",
    "    for chunk in pd.read_csv(big_csv_path, usecols=usecols, chunksize=chunksize):\n",
    "        read_total += len(chunk)\n",
    "        chunk[\"pid\"] = chunk[\"pid\"].astype(str)\n",
    "        sub = chunk[chunk[\"pid\"].isin(pids)]\n",
    "        if sub.empty:\n",
    "            continue\n",
    "\n",
    "        sub.to_csv(out_attr_csv, mode=\"w\" if first else \"a\", index=False, header=first)\n",
    "        first = False\n",
    "        kept += len(sub)\n",
    "\n",
    "    return kept, read_total\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) Décomposition ASC/DESC : solve 2x2 -> Ve, Vv + cond filter\n",
    "# ============================================================\n",
    "def decompose_asc_desc(pairs, cond_max=50.0):\n",
    "    Ve = np.full(len(pairs), np.nan, float)\n",
    "    Vv = np.full(len(pairs), np.nan, float)\n",
    "    cond = np.full(len(pairs), np.nan, float)\n",
    "\n",
    "    for i, row in enumerate(pairs.itertuples(index=False)):\n",
    "        A = np.array([[row.los_east_left,  row.los_up_left],\n",
    "                      [row.los_east_right, row.los_up_right]], dtype=float)\n",
    "        b = np.array([row.mean_velocity_left, row.mean_velocity_right], dtype=float)\n",
    "\n",
    "        if not (np.isfinite(A).all() and np.isfinite(b).all()):\n",
    "            continue\n",
    "\n",
    "        c = np.linalg.cond(A)\n",
    "        cond[i] = c\n",
    "        if (not np.isfinite(c)) or (c > cond_max):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            sol = np.linalg.solve(A, b)  # [Ve, Vv]\n",
    "            Ve[i], Vv[i] = sol[0], sol[1]\n",
    "        except np.linalg.LinAlgError:\n",
    "            pass\n",
    "\n",
    "    out = pairs.copy()\n",
    "    out[\"V_E\"] = Ve\n",
    "    out[\"V_V\"] = Vv\n",
    "    out[\"cond\"] = cond\n",
    "    return out\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7) Plot panels basemap (unique échelle symétrique)\n",
    "# ============================================================\n",
    "def sym_norm(series, p=(2, 98)):\n",
    "    s = pd.to_numeric(series, errors=\"coerce\")\n",
    "    lo, hi = np.nanpercentile(s, p)\n",
    "    vabs = max(abs(lo), abs(hi))\n",
    "    return mcolors.TwoSlopeNorm(vmin=-vabs, vcenter=0, vmax=vabs), vabs\n",
    "\n",
    "def add_basemap(ax, zoom=19, alpha=0.25):\n",
    "    ctx.add_basemap(\n",
    "        ax,\n",
    "        source=ctx.providers.CartoDB.Positron,\n",
    "        zoom=min(zoom, 20),\n",
    "        alpha=alpha,\n",
    "        attribution=False,\n",
    "        reset_extent=False,\n",
    "        zorder=0\n",
    "    )\n",
    "\n",
    "def plot_panels_map(\n",
    "    gdf_3857,\n",
    "    gdf_building_3857,\n",
    "    var,\n",
    "    unit,\n",
    "    title,\n",
    "    periods=(\"2015_2021\", \"2018_2022\", \"2019_2023\"),\n",
    "    pclip=(2, 98),\n",
    "    zoom=19,\n",
    "    point_size=45,\n",
    "    point_alpha=0.9,\n",
    "    pad_m=10,\n",
    "    out_png=None,\n",
    "):\n",
    "    norm, vabs = sym_norm(gdf_3857[var], pclip)\n",
    "\n",
    "    xmin, ymin, xmax, ymax = gdf_building_3857.total_bounds\n",
    "    xmin -= pad_m; ymin -= pad_m; xmax += pad_m; ymax += pad_m\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(periods), figsize=(21, 8), constrained_layout=True)\n",
    "    sc = None\n",
    "\n",
    "    for ax, per in zip(axes, periods):\n",
    "        sub = gdf_3857[gdf_3857[\"period\"] == per]\n",
    "\n",
    "        ax.set_xlim(xmin, xmax)\n",
    "        ax.set_ylim(ymin, ymax)\n",
    "        ax.set_aspect(\"equal\")\n",
    "        ax.set_axis_off()\n",
    "\n",
    "        add_basemap(ax, zoom=zoom, alpha=0.25)\n",
    "\n",
    "        gdf_building_3857.boundary.plot(ax=ax, color=\"black\", linewidth=1.2, zorder=2)\n",
    "\n",
    "        sc = ax.scatter(\n",
    "            sub.geometry.x,\n",
    "            sub.geometry.y,\n",
    "            c=sub[var],\n",
    "            cmap=\"coolwarm\",\n",
    "            norm=norm,\n",
    "            s=point_size,\n",
    "            alpha=point_alpha,\n",
    "            edgecolor=\"black\",\n",
    "            linewidth=0.25,\n",
    "            zorder=5\n",
    "        )\n",
    "\n",
    "        ax.set_title(per, fontsize=16)\n",
    "\n",
    "        vprint(f\"[PLOT] {var} {per}: n={len(sub)}  nan={sub[var].isna().sum()}\")\n",
    "\n",
    "    cbar = fig.colorbar(sc, ax=axes, shrink=0.85, pad=0.02)\n",
    "    cbar.set_label(f\"{var} ({unit}) |max|≈{vabs:.1f}\", fontsize=14)\n",
    "\n",
    "    fig.suptitle(title, fontsize=20)\n",
    "\n",
    "    if out_png:\n",
    "        ensure_dir(os.path.dirname(out_png) or \".\")\n",
    "        fig.savefig(out_png, dpi=200, bbox_inches=\"tight\")\n",
    "        vprint(\"[SAVE]\", out_png)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8) PIPELINE\n",
    "# ============================================================\n",
    "# --- dirs\n",
    "ensure_dir(OUT_DIR)\n",
    "ensure_dir(OUT_PIDS)\n",
    "ensure_dir(OUT_ATTR)\n",
    "ensure_dir(OUT_FIGS)\n",
    "\n",
    "# --- base dir + extents\n",
    "base_dir = find_base_dir(BASE_DIR_CANDIDATES)\n",
    "extents_path = os.path.join(base_dir, EXTENTS_GPKG_NAME)\n",
    "assert_exists(extents_path, \"EXTENTS_GPKG\")\n",
    "\n",
    "# --- AOI\n",
    "b3035, poly3035, bbox3035, poly3035_prep = build_hybrid_aoi_bbox_3035(BUILDING_GPKG, KML_PATH)\n",
    "\n",
    "# --- tiles\n",
    "tiles = select_tiles(extents_path, bbox3035)\n",
    "tiles_l2a = tiles[tiles[\"File Identifier\"].astype(str).str.contains(\"L2a\", na=False)].copy()\n",
    "\n",
    "vprint(\"\\n[TILES] total intersect:\", len(tiles), \"| L2a:\", len(tiles_l2a))\n",
    "vprint(tiles_l2a[\"File Identifier\"].head(10).to_list())\n",
    "\n",
    "# --- 1) PID extraction per tile\n",
    "for fid in tiles_l2a[\"File Identifier\"].tolist():\n",
    "    csv_path = os.path.join(base_dir, fid + \".csv\")\n",
    "    safe = fid.replace(\"/\", \"_\")\n",
    "    out_pids = os.path.join(OUT_PIDS, f\"{safe}_PIDS.csv\")\n",
    "\n",
    "    if not os.path.exists(csv_path):\n",
    "        vprint(\"[MISS CSV]\", fid)\n",
    "        continue\n",
    "\n",
    "    if os.path.exists(out_pids):\n",
    "        vprint(\"[SKIP PIDS]\", safe)\n",
    "        continue\n",
    "\n",
    "    n_kept, n_read = extract_zone_pids(csv_path, out_pids, bbox3035, poly3035_prep, CHUNKSIZE)\n",
    "    vprint(f\"[PIDS] {safe}: kept={n_kept} (read={n_read}) -> {out_pids}\")\n",
    "\n",
    "# --- 2) ATTR extraction per tile (based on its PID list)\n",
    "for fid in tiles_l2a[\"File Identifier\"].tolist():\n",
    "    csv_path = os.path.join(base_dir, fid + \".csv\")\n",
    "    safe = fid.replace(\"/\", \"_\")\n",
    "\n",
    "    pids_csv = os.path.join(OUT_PIDS, f\"{safe}_PIDS.csv\")\n",
    "    out_attr = os.path.join(OUT_ATTR, f\"{safe}_ATTR.csv\")\n",
    "\n",
    "    if not os.path.exists(csv_path):\n",
    "        continue\n",
    "    if not os.path.exists(pids_csv):\n",
    "        vprint(\"[NO PIDS]\", safe)\n",
    "        continue\n",
    "    if os.path.exists(out_attr):\n",
    "        vprint(\"[SKIP ATTR]\", safe)\n",
    "        continue\n",
    "\n",
    "    n_kept, n_read = extract_attrs_for_pids(csv_path, pids_csv, out_attr, CHUNKSIZE)\n",
    "    vprint(f\"[ATTR] {safe}: kept={n_kept} (read={n_read}) -> {out_attr}\")\n",
    "\n",
    "# --- 3) Load ATTR -> gdf_l2a_3035\n",
    "attr_paths = sorted(glob.glob(os.path.join(OUT_ATTR, \"*_ATTR.csv\")))\n",
    "vprint(\"\\n[ATTR] files:\", len(attr_paths))\n",
    "if len(attr_paths) == 0:\n",
    "    raise RuntimeError(\"No ATTR files produced. Check AOI, extents, paths, or CSV availability.\")\n",
    "\n",
    "dfs = []\n",
    "for p in attr_paths:\n",
    "    safe = os.path.basename(p).replace(\"_ATTR.csv\", \"\")\n",
    "    df = pd.read_csv(p)\n",
    "    df[\"tile_safe\"] = safe\n",
    "    df[\"period\"] = period_from_tile_safe(safe)\n",
    "    df[\"orbit\"]  = orbit_from_tile_safe(safe)\n",
    "    dfs.append(df)\n",
    "\n",
    "df_l2a = pd.concat(dfs, ignore_index=True)\n",
    "summarize_df(df_l2a, \"df_l2a\")\n",
    "\n",
    "gdf_l2a_3035 = gpd.GeoDataFrame(\n",
    "    df_l2a,\n",
    "    geometry=gpd.points_from_xy(df_l2a[\"easting\"], df_l2a[\"northing\"]),\n",
    "    crs=\"EPSG:3035\"\n",
    ")\n",
    "summarize_gdf(gdf_l2a_3035, \"gdf_l2a_3035\")\n",
    "\n",
    "vprint(\"\\n[COUNTS] by period/orbit:\")\n",
    "vprint(gdf_l2a_3035.groupby([\"period\",\"orbit\"]).size())\n",
    "\n",
    "# --- 4) Pair + decompose per period -> concat gdf_vel\n",
    "gdf_vel_list = []\n",
    "\n",
    "for period in PERIODS:\n",
    "    sub  = gdf_l2a_3035[gdf_l2a_3035[\"period\"] == period]\n",
    "    asc  = sub[sub[\"orbit\"] == \"ASC\"]\n",
    "    desc = sub[sub[\"orbit\"] == \"DESC\"]\n",
    "\n",
    "    vprint(f\"\\n=== PERIOD {period} === ASC={len(asc)}  DESC={len(desc)}\")\n",
    "    if len(asc) == 0 or len(desc) == 0:\n",
    "        vprint(\"[WARN] missing ASC or DESC -> skip\", period)\n",
    "        continue\n",
    "\n",
    "    pairs = sjoin_nearest(\n",
    "        asc[[\"pid\",\"mean_velocity\",\"los_east\",\"los_up\",\"geometry\"]],\n",
    "        desc[[\"pid\",\"mean_velocity\",\"los_east\",\"los_up\",\"geometry\"]],\n",
    "        how=\"inner\",\n",
    "        max_distance=MAX_DIST_M,\n",
    "        distance_col=\"dist_m\",\n",
    "    )\n",
    "    vprint(\"[PAIRS] raw:\", len(pairs))\n",
    "\n",
    "    # enforce 1-1: keep smallest dist both sides\n",
    "    pairs = pairs.sort_values(\"dist_m\").drop_duplicates(subset=[\"pid_right\"], keep=\"first\")\n",
    "    pairs = pairs.sort_values(\"dist_m\").drop_duplicates(subset=[\"pid_left\"], keep=\"first\")\n",
    "    vprint(\"[PAIRS] 1-1:\", len(pairs), \"| dist(min/med/max) =\",\n",
    "           float(pairs[\"dist_m\"].min()), float(pairs[\"dist_m\"].median()), float(pairs[\"dist_m\"].max()))\n",
    "\n",
    "    pairs = decompose_asc_desc(pairs, cond_max=COND_MAX)\n",
    "\n",
    "    vprint(\"[DECOMP] nan(V_V) =\", int(pairs[\"V_V\"].isna().sum()),\n",
    "           \"| nan(V_E) =\", int(pairs[\"V_E\"].isna().sum()),\n",
    "           \"| cond>max =\", int((pairs[\"cond\"] > COND_MAX).sum()))\n",
    "\n",
    "    gdfv = gpd.GeoDataFrame(\n",
    "        pairs[[\"pid_left\",\"pid_right\",\"dist_m\",\"V_E\",\"V_V\",\"cond\",\"geometry\"]],\n",
    "        geometry=\"geometry\",\n",
    "        crs=\"EPSG:3035\"\n",
    "    ).rename(columns={\"pid_left\":\"pid_asc\",\"pid_right\":\"pid_desc\"})\n",
    "    gdfv[\"period\"] = period\n",
    "    gdf_vel_list.append(gdfv)\n",
    "\n",
    "if len(gdf_vel_list) == 0:\n",
    "    raise RuntimeError(\"No period produced pairs. Check MAX_DIST_M, orbit mapping, AOI content.\")\n",
    "\n",
    "gdf_vel = pd.concat(gdf_vel_list, ignore_index=True)\n",
    "summarize_gdf(gdf_vel, \"gdf_vel\")\n",
    "\n",
    "# --- 5) Cumuls\n",
    "gdf_cum_all = gdf_vel.copy()\n",
    "gdf_cum_all[\"duration_y\"] = gdf_cum_all[\"period\"].map(DURATION_Y).astype(float)\n",
    "gdf_cum_all[\"dV_mm\"] = gdf_cum_all[\"V_V\"].astype(float) * gdf_cum_all[\"duration_y\"]\n",
    "gdf_cum_all[\"dE_mm\"] = gdf_cum_all[\"V_E\"].astype(float) * gdf_cum_all[\"duration_y\"]\n",
    "\n",
    "vprint(\"\\n[CUM] duration_y counts:\")\n",
    "vprint(gdf_cum_all[\"duration_y\"].value_counts(dropna=False))\n",
    "\n",
    "# --- 6) Reproject once for plots\n",
    "gdf_vel_3857 = gdf_vel.to_crs(3857)\n",
    "gdf_cum_all_3857 = gdf_cum_all.to_crs(3857)\n",
    "gdf_building_3857 = b3035.to_crs(3857)\n",
    "\n",
    "# --- 7) Plots + exports\n",
    "plot_panels_map(\n",
    "    gdf_vel_3857, gdf_building_3857,\n",
    "    var=\"V_V\", unit=\"mm/an\",\n",
    "    title=\"Vitesse verticale Vv (mm/an) — décomposition ASC/DESC\",\n",
    "    out_png=os.path.join(OUT_FIGS, \"Vv_mm_per_year_panels.png\"),\n",
    "    point_size=45,\n",
    ")\n",
    "\n",
    "plot_panels_map(\n",
    "    gdf_vel_3857, gdf_building_3857,\n",
    "    var=\"V_E\", unit=\"mm/an\",\n",
    "    title=\"Vitesse Est–Ouest Ve (mm/an) — décomposition ASC/DESC\",\n",
    "    out_png=os.path.join(OUT_FIGS, \"Ve_mm_per_year_panels.png\"),\n",
    "    point_size=45,\n",
    ")\n",
    "\n",
    "plot_panels_map(\n",
    "    gdf_cum_all_3857, gdf_building_3857,\n",
    "    var=\"dV_mm\", unit=\"mm\",\n",
    "    title=\"Déplacement cumulé vertical ΔV (mm) — approx Vv × durée\",\n",
    "    out_png=os.path.join(OUT_FIGS, \"Cum_Vert_mm_panels_basemap.png\"),\n",
    "    point_size=60,\n",
    ")\n",
    "\n",
    "plot_panels_map(\n",
    "    gdf_cum_all_3857, gdf_building_3857,\n",
    "    var=\"dE_mm\", unit=\"mm\",\n",
    "    title=\"Déplacement cumulé Est–Ouest ΔE (mm) — approx Ve × durée\",\n",
    "    out_png=os.path.join(OUT_FIGS, \"Cum_East_mm_panels_basemap.png\"),\n",
    "    point_size=60,\n",
    ")\n",
    "\n",
    "# --- 8) Exports GPKG\n",
    "ensure_dir(os.path.dirname(OUT_GPKG) or \".\")\n",
    "gdf_vel.to_file(OUT_GPKG, layer=\"vel_mm_per_year\", driver=\"GPKG\")\n",
    "gdf_cum_all.to_file(OUT_GPKG, layer=\"cum_mm\", driver=\"GPKG\")\n",
    "b3035.to_file(OUT_GPKG, layer=\"building_roi\", driver=\"GPKG\")\n",
    "\n",
    "vprint(\"\\n[EXPORT] written:\", OUT_GPKG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6b16ce-a5b3-4c3e-8843-b855f9e03fee",
   "metadata": {},
   "source": [
    "# Plot Map + Séries Temporelles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ae9d3e-0118-4b52-9aa3-8f3388918561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9) EXTREMES + EXPORT TS + FIGURE MAP+TS (à coller après pipeline)\n",
    "# Dépend de: gdf_vel, gdf_building_3857, tiles_l2a, base_dir\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Params\n",
    "# -----------------------------\n",
    "PERIODS        = [\"2015_2021\", \"2018_2022\", \"2019_2023\"]\n",
    "PERIOD_ORDER   = [\"2015_2021\", \"2018_2022\", \"2019_2023\"]\n",
    "PERIOD_COLORS  = {\"2015_2021\":\"#1f77b4\", \"2018_2022\":\"#ff7f0e\", \"2019_2023\":\"#2ca02c\"}\n",
    "DURATION_Y     = {\"2015_2021\": 7.0, \"2018_2022\": 5.0, \"2019_2023\": 5.0}\n",
    "\n",
    "ZOOM   = 20\n",
    "MAP_PT_SIZE = 35\n",
    "MAP_PT_EDGE = 0.2\n",
    "MAP_PAD     = 25\n",
    "\n",
    "OUT_TS_EXT = \"egms_extract_zone_ts_extremes\"\n",
    "os.makedirs(OUT_TS_EXT, exist_ok=True)\n",
    "TS_CSV = os.path.join(OUT_TS_EXT, \"ts_extremes_long.csv\")\n",
    "\n",
    "# Colonne à analyser (cumul vertical)\n",
    "VAR = \"cum_Vv_mm\"\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def period_from_fid(fid: str) -> str:\n",
    "    fid = str(fid)\n",
    "    if \"2018_2022\" in fid: return \"2018_2022\"\n",
    "    if \"2019_2023\" in fid: return \"2019_2023\"\n",
    "    return \"2015_2021\"\n",
    "\n",
    "def get_extreme_row(gdf, period, var=VAR):\n",
    "    sub = gdf[gdf[\"period\"] == period].dropna(subset=[var, \"pid_asc\", \"pid_desc\"])\n",
    "    if sub.empty:\n",
    "        return None\n",
    "    return sub.loc[sub[var].abs().idxmax()]\n",
    "\n",
    "def sym_norm(series, p=(2,98)):\n",
    "    series = pd.to_numeric(series, errors=\"coerce\")\n",
    "    lo, hi = np.nanpercentile(series, p)\n",
    "    vabs = max(abs(lo), abs(hi))\n",
    "    return mcolors.TwoSlopeNorm(vmin=-vabs, vcenter=0, vmax=vabs), vabs\n",
    "\n",
    "def get_date_cols(cols):\n",
    "    return [c for c in cols if isinstance(c, str) and c.isdigit() and len(c) == 8]\n",
    "\n",
    "def export_extreme_ts_all_tiles(tiles_l2a, base_dir, pids_ext, out_csv_path, chunksize=200_000):\n",
    "    \"\"\"\n",
    "    Exporte les TS LOS des PIDs extrêmes, pour toutes les tuiles (toutes périodes),\n",
    "    en format long: pid, date, value, fid.\n",
    "    \"\"\"\n",
    "    pids_ext = set(map(str, pids_ext))\n",
    "\n",
    "    if os.path.exists(out_csv_path):\n",
    "        os.remove(out_csv_path)\n",
    "\n",
    "    first_write = True\n",
    "    total_rows = 0\n",
    "\n",
    "    for fid in tiles_l2a[\"File Identifier\"].tolist():\n",
    "        csv_path = os.path.join(base_dir, fid + \".csv\")\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(\"[MISS]\", fid)\n",
    "            continue\n",
    "\n",
    "        head = pd.read_csv(csv_path, nrows=1)\n",
    "        date_cols = get_date_cols(head.columns)\n",
    "        if not date_cols:\n",
    "            print(\"[NO DATE COLS]\", fid)\n",
    "            continue\n",
    "\n",
    "        usecols = [\"pid\"] + date_cols\n",
    "        kept_tile = 0\n",
    "\n",
    "        for chunk in pd.read_csv(csv_path, usecols=usecols, chunksize=chunksize):\n",
    "            chunk[\"pid\"] = chunk[\"pid\"].astype(str)\n",
    "            chunk = chunk[chunk[\"pid\"].isin(pids_ext)]\n",
    "            if chunk.empty:\n",
    "                continue\n",
    "\n",
    "            df_long = chunk.melt(\n",
    "                id_vars=[\"pid\"],\n",
    "                value_vars=date_cols,\n",
    "                var_name=\"date\",\n",
    "                value_name=\"value\"\n",
    "            )\n",
    "\n",
    "            df_long[\"date\"]  = pd.to_datetime(df_long[\"date\"], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "            df_long[\"value\"] = pd.to_numeric(df_long[\"value\"], errors=\"coerce\")\n",
    "            df_long = df_long.dropna(subset=[\"date\",\"value\"])\n",
    "\n",
    "            df_long[\"fid\"] = fid\n",
    "\n",
    "            df_long.to_csv(\n",
    "                out_csv_path,\n",
    "                mode=\"w\" if first_write else \"a\",\n",
    "                index=False,\n",
    "                header=first_write\n",
    "            )\n",
    "            first_write = False\n",
    "\n",
    "            kept_tile += len(df_long)\n",
    "            total_rows += len(df_long)\n",
    "\n",
    "        print(\"[TS]\", fid, \"->\", kept_tile)\n",
    "\n",
    "    print(\"DONE. total rows:\", total_rows)\n",
    "    print(\"Saved:\", out_csv_path)\n",
    "    return total_rows\n",
    "\n",
    "\n",
    "def chained_offsets(df_pid, period_order=PERIOD_ORDER, min_overlap=3):\n",
    "    \"\"\"\n",
    "    Chaînage simple: p0 ref, p1 alignée sur p0, p2 alignée sur p1, etc.\n",
    "    \"\"\"\n",
    "    offsets = {period_order[0]: 0.0}\n",
    "    for p_prev, p_cur in zip(period_order[:-1], period_order[1:]):\n",
    "        d_prev = df_pid[df_pid[\"period\"] == p_prev][[\"date\",\"value\"]]\n",
    "        d_cur  = df_pid[df_pid[\"period\"] == p_cur ][[\"date\",\"value\"]]\n",
    "\n",
    "        overlap = np.intersect1d(d_prev[\"date\"].to_numpy(), d_cur[\"date\"].to_numpy())\n",
    "        if len(overlap) < min_overlap:\n",
    "            offsets[p_cur] = offsets[p_prev]\n",
    "            continue\n",
    "\n",
    "        v_prev = d_prev[d_prev[\"date\"].isin(overlap)][\"value\"].to_numpy()\n",
    "        v_cur  = d_cur [d_cur [\"date\"].isin(overlap)][\"value\"].to_numpy()\n",
    "        delta  = np.median(v_prev - v_cur)\n",
    "\n",
    "        offsets[p_cur] = offsets[p_prev] + delta\n",
    "\n",
    "    return offsets\n",
    "\n",
    "def compute_global_ylim(df_ts, pids, period_order=PERIOD_ORDER, min_overlap=3):\n",
    "    ymin, ymax = np.inf, -np.inf\n",
    "    for pid in pids:\n",
    "        sub = df_ts[df_ts[\"pid\"] == pid]\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        off = chained_offsets(sub, period_order=period_order, min_overlap=min_overlap)\n",
    "        for p in period_order:\n",
    "            sp = sub[sub[\"period\"] == p]\n",
    "            if sp.empty:\n",
    "                continue\n",
    "            v = sp[\"value\"].to_numpy() + off.get(p, 0.0)\n",
    "            ymin = min(ymin, np.nanmin(v))\n",
    "            ymax = max(ymax, np.nanmax(v))\n",
    "\n",
    "    if not np.isfinite(ymin) or not np.isfinite(ymax):\n",
    "        return None\n",
    "    pad = 0.05 * (ymax - ymin if ymax > ymin else 1.0)\n",
    "    return (ymin - pad, ymax + pad)\n",
    "\n",
    "def plot_pid_chain(ax, df_ts, pid, title,\n",
    "                   period_order=PERIOD_ORDER,\n",
    "                   xlim=None, ylim=None,\n",
    "                   show_legend=True,\n",
    "                   show_xticks=True):\n",
    "\n",
    "    sub = df_ts[df_ts[\"pid\"] == pid].copy()\n",
    "    if sub.empty:\n",
    "        ax.text(0.5,0.5,\"No TS\",ha=\"center\",va=\"center\",transform=ax.transAxes)\n",
    "        return\n",
    "\n",
    "    off = chained_offsets(sub, period_order=period_order, min_overlap=3)\n",
    "\n",
    "    for p in period_order:\n",
    "        sp = sub[sub[\"period\"] == p].sort_values(\"date\")\n",
    "        if sp.empty:\n",
    "            continue\n",
    "        ax.plot(sp[\"date\"], sp[\"value\"] + off[p],\n",
    "                lw=1.2, color=PERIOD_COLORS[p], label=p)\n",
    "\n",
    "    ax.text(0.01, 0.02, title,\n",
    "            transform=ax.transAxes, ha=\"left\", va=\"bottom\",\n",
    "            fontsize=11,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.2\",\n",
    "                      facecolor=\"white\", alpha=0.7, linewidth=0))\n",
    "\n",
    "    ax.set_ylabel(\"LOS (mm)\")\n",
    "    ax.grid(True, ls=\"--\", alpha=0.3)\n",
    "\n",
    "    if show_legend:\n",
    "        ax.legend(fontsize=8, loc=\"upper right\")\n",
    "\n",
    "    if xlim is not None: ax.set_xlim(*xlim)\n",
    "    if ylim is not None: ax.set_ylim(*ylim)\n",
    "\n",
    "    if not show_xticks:\n",
    "        ax.tick_params(axis=\"x\", which=\"both\", labelbottom=False)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Construire la variable extrême sur gdf_vel\n",
    "# -----------------------------\n",
    "gdf_ext = gdf_vel.copy()\n",
    "gdf_ext[\"duration_y\"] = gdf_ext[\"period\"].map(DURATION_Y).astype(float)\n",
    "gdf_ext[VAR] = gdf_ext[\"V_V\"].astype(float) * gdf_ext[\"duration_y\"]\n",
    "\n",
    "# CRS carte\n",
    "gdf_3857 = gdf_ext.to_crs(3857) if (gdf_ext.crs and gdf_ext.crs.to_epsg() != 3857) else gdf_ext\n",
    "b_3857 = gdf_building_3857\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Trouver PIDs extrêmes (ASC/DESC) par période\n",
    "# -----------------------------\n",
    "ext_rows = {}\n",
    "pids_ext = set()\n",
    "\n",
    "for per in PERIODS:\n",
    "    row = get_extreme_row(gdf_3857, per, var=VAR)\n",
    "    if row is None:\n",
    "        print(\"[WARN] no extreme row:\", per)\n",
    "        continue\n",
    "    ext_rows[per] = row\n",
    "    pids_ext.add(str(row[\"pid_asc\"]))\n",
    "    pids_ext.add(str(row[\"pid_desc\"]))\n",
    "\n",
    "print(\"Extreme PIDs:\", sorted(pids_ext))\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Export TS des PIDs extrêmes (si pas déjà fait)\n",
    "# -----------------------------\n",
    "if not os.path.exists(TS_CSV):\n",
    "    n = export_extreme_ts_all_tiles(\n",
    "        tiles_l2a=tiles_l2a,\n",
    "        base_dir=base_dir,\n",
    "        pids_ext=pids_ext,\n",
    "        out_csv_path=TS_CSV,\n",
    "        chunksize=200_000\n",
    "    )\n",
    "    print(\"TS rows:\", n)\n",
    "else:\n",
    "    print(\"[SKIP] TS already exists:\", TS_CSV)\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Charger TS + xlim/ylim global\n",
    "# -----------------------------\n",
    "df_ts = pd.read_csv(TS_CSV)\n",
    "df_ts[\"date\"] = pd.to_datetime(df_ts[\"date\"])\n",
    "df_ts[\"pid\"]  = df_ts[\"pid\"].astype(str)\n",
    "df_ts[\"period\"] = df_ts[\"fid\"].apply(period_from_fid)\n",
    "\n",
    "xmin = df_ts[\"date\"].min()\n",
    "xmax = df_ts[\"date\"].max()\n",
    "xlim_global = (xmin - pd.DateOffset(months=6), xmax + pd.DateOffset(months=6))\n",
    "\n",
    "ylim_global = compute_global_ylim(df_ts, pids_ext)\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Figure finale\n",
    "# -----------------------------\n",
    "def plot_complex(periods=PERIODS, var=VAR, zoom=ZOOM,\n",
    "                 xlim_ts=xlim_global, ylim_ts=ylim_global):\n",
    "\n",
    "    norm, vabs = sym_norm(gdf_3857[var], p=(2,98))\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 12))\n",
    "    gs = fig.add_gridspec(\n",
    "        nrows=len(periods), ncols=2,\n",
    "        width_ratios=[1.0, 1.6],\n",
    "        hspace=0.35, wspace=0.15\n",
    "    )\n",
    "\n",
    "    sc_for_cbar = None\n",
    "\n",
    "    for i, per in enumerate(periods):\n",
    "        r = ext_rows.get(per)\n",
    "        if r is None:\n",
    "            continue\n",
    "\n",
    "        pid_a = str(r[\"pid_asc\"])\n",
    "        pid_d = str(r[\"pid_desc\"])\n",
    "\n",
    "        # ---- MAP (gauche)\n",
    "        axm = fig.add_subplot(gs[i, 0])\n",
    "        sub = gdf_3857[gdf_3857[\"period\"] == per]\n",
    "\n",
    "        b_3857.plot(ax=axm, facecolor=\"none\", edgecolor=\"black\", linewidth=1.2)\n",
    "        sc = axm.scatter(\n",
    "            sub.geometry.x, sub.geometry.y,\n",
    "            c=sub[var], cmap=\"coolwarm\", norm=norm,\n",
    "            s=MAP_PT_SIZE, edgecolor=\"black\", linewidth=MAP_PT_EDGE\n",
    "        )\n",
    "        if sc_for_cbar is None:\n",
    "            sc_for_cbar = sc\n",
    "\n",
    "        axm.scatter([r.geometry.x], [r.geometry.y],\n",
    "                    s=220, facecolor=\"none\", edgecolor=\"black\", linewidth=2)\n",
    "\n",
    "        axm.set_title(f\"{per} — EXT | {var}={float(r[var]):.1f} mm\")\n",
    "        axm.set_axis_off()\n",
    "        axm.set_aspect(\"equal\")\n",
    "\n",
    "        xminb, yminb, xmaxb, ymaxb = b_3857.total_bounds\n",
    "        axm.set_xlim(xminb - MAP_PAD, xmaxb + MAP_PAD)\n",
    "        axm.set_ylim(yminb - MAP_PAD, ymaxb + MAP_PAD)\n",
    "\n",
    "        ctx.add_basemap(axm, source=ctx.providers.CartoDB.Positron,\n",
    "                        zoom=zoom, attribution=False)\n",
    "\n",
    "        # ---- TS panel (droite) : 2 sous-axes\n",
    "        axt = fig.add_subplot(gs[i, 1])\n",
    "        pos = axt.get_position()\n",
    "        axt.remove()\n",
    "\n",
    "        ax1 = fig.add_axes([pos.x0, pos.y0 + pos.height*0.52, pos.width, pos.height*0.48])\n",
    "        ax2 = fig.add_axes([pos.x0, pos.y0,                 pos.width, pos.height*0.48])\n",
    "\n",
    "        plot_pid_chain(ax1, df_ts, pid_a, f\"ASC {pid_a}\",\n",
    "                       xlim=xlim_ts, ylim=ylim_ts, show_legend=True, show_xticks=False)\n",
    "        plot_pid_chain(ax2, df_ts, pid_d, f\"DESC {pid_d}\",\n",
    "                       xlim=xlim_ts, ylim=ylim_ts, show_legend=True, show_xticks=True)\n",
    "        ax2.set_xlabel(\"Date\")\n",
    "\n",
    "    if sc_for_cbar is not None:\n",
    "        cbar = fig.colorbar(sc_for_cbar, ax=fig.axes, shrink=0.6, pad=0.04, fraction=0.03)\n",
    "        cbar.set_label(f\"{var} (mm)  |max|≈{vabs:.1f}\")\n",
    "\n",
    "    plt.suptitle(\"Extremes cum_Vv_mm — map + TS ASC/DESC (superposition chainée)\", y=0.995)\n",
    "    plt.show()\n",
    "\n",
    "plot_complex()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75821489-334c-4846-aabb-1fbae5ec8398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5cd418-ff19-4b44-88fe-a2fa55260223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3339623-71e6-4f48-a301-aa2916bc4914",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mapchallenge)",
   "language": "python",
   "name": "mapchallenge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
